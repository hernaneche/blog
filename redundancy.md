#Redundancy In Programming Languages   

_"Everything should be made as simple as possible, but not simpler."_-A. Einstein

**Minimalism And Redundancy**  

Exact sciences writing can have a very low resilience, a single mistake in a equation sign can lead to totally wrong results, paradoxically this fragility is brought by minimalism, which mathematicians pursue.
So, how to prevent single failures from tearing down a whole system ? The obvious trick for resilience is propagation, i.e. distributed replication, it's like making a backup of data and putting copies in different places, this is not the same thing as making data larger; because it's the same data which spreads, the aim is to keep conceptual minimalism, gaining resilience while preserving the name and the work behind it, that's more or less the idea, that any data pattern could be propagated independent of _how_ the storing/communication is implemented for it, so _adding a layer with some level of redundancy fosters resilience in the upper layer_.  
For example in safety-critical systems some parts may be triplicated, all three working in parallel doing just the same, so the output is not decided by none of them alone but by a quorum scheme (a two of three voting system), if one of those part fails the system will decide right anyway, avoiding sporadic failures effects and giving time to find the cause; a similar technique is to have spare parts of vital subsystems connected and ready to work as Redundancy, so they can auto-replace a failing partner without intervention (for example when a power supply fails, a second one could replaces it on the fly, preventing the system shutdown, and again allowing time to review the cause of the first failure). Note that we have redundant organs, like two lungs, two kidneys, two ovaries, two testicles, and so on, these techniques could be independent of what the system is, what the system do, and whatever design and functionality it has on top of it, we could start with a design already beautiful or efficient but fragile, and _then_ make it resilent by adding some independent redundancy layer.

**Computer Things**   

Computer Programs do have low redundancy, if we choose to type random characters when programming a source code, we will get into a bug with very high probability! (at the best of cases it won't even compile), programming languages often have a poor [Hamming distance](http://en.wikipedia.org/wiki/Hamming_distance) between any intended functional code and a bug, this low redundancy is partly inherited from requirements specification, whatever the target functionality is, there is a narrow amount of programs that fulfill it, it's exquisite within a large space of wrong posibilities, i.e. we don't settle with most of possible programs, but the low redundancy is also _intentional_ ([DRY Principle](http://en.wikipedia.org/wiki/Don%27t_repeat_yourself)), except for things like names, spacing, comments.. the source code itself is fragile, and besides the functionality layer, we try to have the most compact (yet readable) source code representation, redundancy within the code ise viewed as a cause of future problems, because as software evolves (as it -probably- will) it becomes necessary to make changes to it, we want changes and effects to be as localized as possible, i.e. for doing changes about a single feature, we want to edit only a single place, and once (keep the spread to a minimum), also we expect that little change dont pull down the entire system. *This is a very weird goal to have*, why? because we want a system to be ready for our "intentional" changes, to be Easy to be controlled and changed by "us", but at the same time, we want it to be Hard to change by any "unintended" means, like noises, errors, hardware problems, other people, etc. It's kind of asking a system to be able to know what changes are intended by us and which ones are unintended, it seems that what we are willing to have a system that knows the future and even to know our choices in advance, it is a system that "encodes our will", as an extension of us, a system that reflex _our_ desires over time but without telling in advance, it's paradoxical and kind of crazy to ask a system for knowing what we intent, being that sometimes not even we know it!.  
It's usual to define certain sanity criteria for data, the feature of avoiding "unintended" changes sometimes can be implement(but only in the sense of a fixed criteria), in data systems this is known as ECC(Error-Correcting Codes), and there are a lot of algorithms about how to introduce enough redundancy within data to recover it from some errors, against some criteria, but when criteria changes, for example data format changes, or it migrates to a new medium,we are again in problems. Also a [revision control system](http://en.wikipedia.org/wiki/Git_%28software%29) is a high level way of adding resilience to a software project, saving history of changes, important for developing projects that change through time, namely, any project.  
All of those schemes are usually a separate layer from data, i.e. they modify the data format in a way it loses his original coding in a way that it can't be parsed directly as from the raw data source, it has to pass first through a decoder that recover the backups, checks it, sanitize, decode and so on with respect of some resilience criteria.  

**Human Things**  

If you take a Novel book, or even Law book, and then write down some random characters within their pages, perhaps they would still be readable, the meaning of the work could be recovered unchanged or with a minimal damage. In a 1950 paper "Prediction and Entropy of Printed English" Claude Shannon found english text to be 75% redundant, in a specific sense, that same information contained in a word or a sentence can be inferred by using just some related letters (as [text phone language](http://en.wikipedia.org/wiki/SMS_language) and [autocomplete functions](https://en.wikipedia.org/wiki/Autocomplete)  have shown), of course, redundancy could only be useful if the reader knows in advance the statistical distribution of letters, words, sentences, etc. Knowing english words would be enough to hit some predictions at word level, but not at the sentence level, nor at paragraph level, there are other probabilities for every level, because of that an Historian would do better predictions(and error-corrections) on a History book, for a Math book a Mathematician would do it better, and so on. The prior general knowledge about the language let us predict and correct syntax errors, the prior kwnowledge about the subject allows to predict and correct sentences and global errors, because an error by definition is relative to the field or domain of knowledge.  
Resilience in human communication can be traced in the redundancy of evolved languages, its long words, grammatical rules, sentences, context, and so on, but it's a bold human understanding merit too, we do a good job as interpreters, and this is a double-edged sword, while it's difficult to ask a computer program to parse some data, it's impossible to ask a human NOT to interpret data!...["don't think on an elephant"](http://en.wikipedia.org/wiki/Ironic_process_theory) so making better programming languages is about getting better grammar and better compilers/interpreters, resilience is defined not only within the syntax but also by the interpreter, both compose a self healing feature that could overcome the presence of noise and mistakes. Talking about redundancy in human language is a too narrow way of see it, resilience on a human message implies much more than redundancy, it's part of the message but it's also in us, our ways and protocols are much more complex, anyway redundancy is there.  

A curiosity to note is that human written languages have redundancy in it's own encoding layer, i.e. written text has syntax clues, grammatical clues, and even semantic clues that let us fix errors and guess what was intended to be said (depending on our prior knowledge and skills), there is no explicit encoder/decoder mechanism. It's like if we are parsing many layer at once, there are no intermediate translation, at least no a conscious one. We speak redundancy indistinguishable from data, for us they are composed together, without coders and decoders, we have no explicit datatype field in our text, still a picture is worth a thousand words, i.e. there are things outside our language.

**Monkey Things**  

Let's make a thought experiment about propagation of same data, imagine a control system of a big dam in a river near a jungle, the system is capable of openning or closing dam's gates, depending on rainfall, also a bad decision on it could lead to flooding, letting thousand people underwater. To be practical and fast enough the system can open or close ALL dam gates from a button in the control box, but you know, being near a jungle has some side effects like monkeys bothering around, even a single monkey entering at night through a window could turn the main switch and produce a flood. To avoid this issue, we need the system to be monkey-resistant, as we saw, a way to gain resilience is to add some kind of redundancy, multiplicity, to spread it, so for example instead of having only one switch to bring them all, we could install three individual switches for every gate, we could even ask for all switch to be turned within a time interval to produce the all-gate movement, this would transform the incident from being imminent to be as unlikely as having multiple monkeys moving three switchs at near the same time. This will increase system safety at expenses of making the operator task harder. But if later we want to command same system from a remote location, from a more secure area, for example a cell phone on a comfortable office building in a city away from monkeys then the three switch system could be not necessary, again the system could be triggered by a single switch leading now the resilience to the phone, and the communication path. This single visible controllable "intention button" is a new layer to manage a hidden multiplicity that should only be accessible from the "intended" zone, it _hides_ complexity of the "unintended" zone. We can hide it **because** we can control it. The brittleness and severity of some dangers could be diluted by propagation, multiplicity and diversity : different people, different techniques, different locations, gain resilience at the expense of complexity growing over the system, but that complexity is not for free, it also has to be managed and this could create a new danger, it would not be any gain if we just exchange the culprit of the damage from a monkey to stressed workers, because the aims is to avoid the damage itself not the guilties, the trick is that the number of people in charge can rise, and also be organized, this hierarchical distribution of work and systems can be socially viewed as the distribution and consequent dilution of each one individual/subsystem damage and responsability, for the enhancement of the system as a whole, its resilience, designers, users and everyone. We could think hierarchy organizations as a by product of component limitations, but also to overcome them. As systems evolve propagated in spacetime, transcends, learning how to organize, keeping track of functional structures, added multiplicity is to avoid single points of failure, also avoiding an overload every component, it seems to "benefits" components security by removing responsibility (but also removes individual functionality), It seems to have no choice, a big system must have little components, if any part have too much responsibility for the whole, everyone can be harm. Minimizing responsibility of every component leads to a better system design in the strict sense of gaining more power to the whole system while trascending through time, losing power of individual components is not a shared intention, especially if we are those "components".
